# GPT-2

GPT-2 is an architecture for [[language_modelling]]. It is a large [[transformer]] model and is primarily used for [[natural_language_generation]]. It was trained with a large general corpus but can adapted with [[fine-tuning]] or [[fine-tuning]] in order to specify a domain for the text that is to be generated, e.g. erotic literature.

### Resources

- https://openai.com/blog/better-language-models/
- https://www.youtube.com/watch?v=T0I88NhR_9M
- https://minimaxir.com/2020/01/twitter-gpt2-bot/
- https://minimaxir.com/2019/09/howto-gpt2/
- https://github.com/minimaxir/aitextgen
- https://github.com/orange-erotic-bible/orange-erotic-bible
- https://www.gwern.net/GPT-2
- https://jalammar.github.io/illustrated-gpt2/
- https://github.com/karpathy/minGPT
- http://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/
- https://bkkaggle.github.io/blog/algpt2/2020/07/17/ALGPT2-part-2.html
- https://github.com/Xirider/finetune-gpt2xl
