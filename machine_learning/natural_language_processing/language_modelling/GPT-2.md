# GPT-2

GPT-2 is an architecture for [[language_modelling]]. It is a large [[transformer]] model and is primarily used for [[natural_language_generation]]. It was trained with a large general corpus but can adapted with [[fine-tuning]] or [[fine-tuning]] in order to specify a domain for the text that is to be generated, e.g. erotic literature.

### Resources

- https://openai.com/blog/better-language-models/
- https://minimaxir.com/2020/01/twitter-gpt2-bot/
- https://minimaxir.com/2019/09/howto-gpt2/
- https://github.com/orange-erotic-bible/orange-erotic-bible