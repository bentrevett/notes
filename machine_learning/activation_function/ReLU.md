# ReLU

The rectified linear unit (ReLU) is an [[activation_function]] used in [[neural_network]] models. It is the most common activation function used.

$$
f(x) = \begin{cases}
0 & \text{for } x \lt 0\\
x & \text{for } x \ge 0
\end{cases}$$