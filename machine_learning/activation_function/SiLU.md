# SiLU

The sigmoid linear unit (SiLU) is an [[activation_function]].

$$f(x) = x \cdot \sigma(x) = \frac{x}{1+e^{-x}}$$

$\sigma(x)$ is the [[sigmoid]] activation function. 

SiLU can be seen as the [[swish]] activation function with $\beta = 1$.