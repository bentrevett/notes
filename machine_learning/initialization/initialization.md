# Initialization

This is how you initialize the weights and biases of your [[neural_network]].

Things to mention: glorot or he - apparently glorot is for tanh/sigmoid and he is for relu.

what about dead [[ReLU]]? need biases initialized to a small pos. constant to prevent dead relus. leaky relus help.

what about mish?

### Resources

- https://arxiv.org/abs/1912.08957