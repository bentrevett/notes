# Automatic Differentiation

Automatic differentiation (AD) is a computer algorithm to calculate the derivative of a function by repeatedly applying the [[chain_rule]].

It is commonly used in machine learning frameworks to perform [[backpropagation]].

### Resources

- https://github.com/karpathy/micrograd
- https://github.com/geohot/tinygrad
- https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation
- https://github.com/joelgrus/joelnet (implementation with videos)
- https://github.com/malwaredllc/autograd-for-dummies
- https://github.com/PABannier/nanograd
